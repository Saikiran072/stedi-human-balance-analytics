# ğŸ§â€â™‚ï¸ STEDI Human Balance Analytics

### Spark & AWS Glue Data Lakehouse Project

---

## ğŸ“Œ Project Introduction

The STEDI Human Balance Analytics project focuses on building a data lakehouse solution using AWS Glue and Apache Spark. The objective is to process raw sensor data, curate trusted datasets, and prepare step trainer data that can be used for machine learning applications.


---

## ğŸ“Œ Project Overview

This project implements a **Data Lakehouse solution on AWS** for STEDI â€” a hardware company that produces a Step Trainer device used for balance training.

As a Data Engineer on the STEDI team, the objective is to:

- Ingest sensor and mobile app data into AWS S3
- Build a structured data lake using AWS Glue & Spark
- Transform raw data into trusted and curated datasets
- Prepare privacy-compliant machine learning training data

The final curated dataset enables Data Scientists to train a machine learning model that accurately detects steps in real-time.

---

## ğŸ—ï¸ Architecture Overview

The project follows a **multi-layered Data Lake architecture**:

Landing Zone (Raw Data - S3)  
 â†“  
Trusted Zone (Cleaned & Validated - S3)  
 â†“  
Curated Zone (ML Ready Data - S3)

---

## ğŸ› ï¸ Technologies Used

- AWS S3
- AWS Glue
- Apache Spark (PySpark)
- AWS IAM
- AWS Lake Formation
- Glue Data Catalog

---

## ğŸ“‚ Dataset Description

STEDI produces three types of data:

### 1ï¸âƒ£ Customer Data

Collected from the mobile application.

Includes:

- Customer name
- Email
- Phone
- Registration details
- Research consent flag

Only customers who agreed to share data for research are used for ML training.

---

### 2ï¸âƒ£ Accelerometer Data

Collected from the mobile device sensors.

Includes:

- Timestamp
- X, Y, Z acceleration values
- User email

---

### 3ï¸âƒ£ Step Trainer Sensor Data

Collected from the STEDI Step Trainer hardware.

Includes:

- Timestamp
- Distance measurement
- Device sensor readings

---

## ğŸ”„ ETL Pipeline Implementation

AWS Glue Jobs were created to transform data across zones.

---

### ğŸ¥‡ 1. Customer Landing â†’ Trusted

Script: `customer_landing_to_trusted.py`

- Filters customers who agreed to share research data
- Removes invalid records
- Writes clean data to Trusted zone

---

### ğŸ¥ˆ 2. Accelerometer Landing â†’ Trusted

Script: `accelerometer_landing_to_trusted.py`

- Joins accelerometer data with trusted customers
- Ensures only consented usersâ€™ data is retained
- Stores clean data in Trusted zone

---

### ğŸ¥‰ 3. Step Trainer Landing â†’ Trusted

Script: `step_trainer_to_trusted.py`

- Cleans raw step trainer sensor data
- Removes corrupt or incomplete records
- Stores validated sensor data

---

### ğŸ§  4. Customer Trusted â†’ Curated

Script: `customer_trusted_to_curated.py`

- Prepares customer dataset for analytical queries
- Optimizes schema for performance

---

### ğŸ¤– 5. ML Curated Dataset Creation

Script: `ml_curated.py`

- Joins:
  - Trusted Accelerometer Data
  - Trusted Step Trainer Data
  - Curated Customer Data

- Produces ML-ready dataset
- Ensures:
  - Only research-consented users included
  - Data is privacy compliant
  - Ready for machine learning training

---

## ğŸ” Privacy & Data Governance

Privacy is a primary requirement in this project.

- Only customers who explicitly agreed to research sharing are included
- Non-consented users are excluded at the trusted layer
- Data is separated into zones to prevent accidental misuse

---

## ğŸ“Š Data Lake Zones Explained

| Zone    | Purpose                              |
| ------- | ------------------------------------ |
| Landing | Raw ingested data from S3            |
| Trusted | Cleaned & validated datasets         |
| Curated | Business-ready and ML-ready datasets |

---

## ğŸ¯ Project Outcome

After completing the ETL pipeline:

- Clean, structured data lakehouse built on AWS
- Privacy-compliant ML training dataset created
- Sensor data curated for machine learning model development
- Scalable Spark-based transformation pipeline implemented
- Data Scientists can now train real-time step detection models

---

## ğŸ’¡ Key Learnings

Through this project, I gained hands-on experience in:

- Building Data Lake architecture on AWS
- Writing AWS Glue ETL jobs using PySpark
- Implementing multi-layer data transformations
- Performing secure data filtering for privacy compliance
- Joining large datasets efficiently in Spark
- Designing ML-ready curated datasets
